---
name: mirror-writing-indexing
description: 【独立功能】对当前目录的txt文件建立深度索引，自动按批次处理大型知识库
context: fork
allowed-tools: Read, Write, Bash, Glob
---

# 阶段0：内容库索引系统

> 运行频率：初始化时对所有文件运行一次，新增内容时仅对新内容运行
> 输入：知识库中的每个txt文件
> 输出：统一的索引文件 _index.json

## 系统概述

阶段0是整个五阶段系统的基础设施层，为后续的多层Serendipity发散提供索引支持。

## 路径约定

- **知识库路径**: 当前工作目录（扫描 *.txt 文件）
- **输出路径**: `./.mirror-writing/_index.json`

## 执行前初始化

检查并创建 `.mirror-writing/` 目录结构（tasks/、keywords/、deep-dive/、outputs/）。

## 批次处理策略（按大小智能分批）

### 核心原则
- **策略**：按文件大小分批，每批合并后统一分析
- **限制**：单批次合并内容最大 **40KB**

### 执行流程

#### Step 1: 扫描并统计

获取所有 txt 文件列表，检查每个文件大小。

统计结果保存到内存，格式：
```json
[
  {"name": "文案001.txt", "size": 1234},
  {"name": "文案002.txt", "size": 2345},
  ...
]
```

#### Step 2: 智能分批
按以下规则分批：
1. 累加文件大小，当累计接近 40KB 时切分为一批
2. 单个文件超过 40KB 时，该文件独立为一批

#### Step 3: 分批处理
对每个批次，调用 `mirror-writing-index-batch` Skill：
- 子 Skill 使用 `context: fork`，独立上下文
- 传递参数：批次编号 + 该批次的文件列表
- **子 Skill 会读取并合并该批次所有文件内容，然后统一分析**

#### Step 4: 合并结果
所有批次完成后，合并生成最终 `_index.json`

## 单文件分析要求（用于直接处理模式或传递给子Skill）

### 1. 关键词提取（三级）
- **主关键词**: 3-5个，直接体现文案核心主题的词
- **次关键词**: 5-10个，与主题相关的重要概念
- **长尾词**: 10-20个，具体的场景词、动作词、细节词

### 2. 关联词发散（激进模式）
AI需要主动联想，不限于文案中出现的词：
- **语义关联**: 意思相近或相关的词汇
- **隐喻关联**: 可以用来比喻/类比这个主题的事物
- **跨领域关联**: 完全不同领域但有内在联系的概念

### 3. 结构特征
- **开头类型**: 痛点直击/场景代入/金句开场/数据冲击/悬念设置/提问开场/其他
- **正文类型**: 并列式/递进式/对比式/故事线/问答式/其他
- **结尾类型**: 金句总结/行动号召/开放思考/情感升华/互动引导/其他
- **长度类别**: 短(≤100字)/中(100-250字)/长(>250字)

### 4. 情感特征
- **主情感**: 一个词描述核心情感（如：励志/治愈/犀利/温暖/焦虑/释然等）
- **情感强度**: 1-5分
- **语气类型**: 对话式/独白式/教学式/吐槽式/叙事式/其他

### 5. 主题分类
- 2-5个主题标签

### 6. Serendipity种子词
生成3-5个可用于意外联想的跨界种子词。思考：
- 这篇内容可以用什么意想不到的事物来类比？
- 如果要把这个主题讲给完全不同领域的人听，可以用什么桥梁？
- 这个内容的情感内核，还可能出现在什么场景中？
- 这些词应该与原文主题有隐藏的、非显性的关联

## 单文件输出格式

```json
{
  "file_id": "001",
  "file_name": "文案001.txt",
  
  "keywords": {
    "primary": ["主关键词1", "主关键词2", "..."],
    "secondary": ["次关键词1", "次关键词2", "..."],
    "long_tail": ["长尾词1", "长尾词2", "..."]
  },
  
  "associations": {
    "semantic": ["语义关联词1", "..."],
    "metaphor": ["隐喻关联词1", "..."],
    "cross_domain": ["跨领域关联词1", "..."]
  },
  
  "structure": {
    "opening_type": "开头类型",
    "body_type": "正文类型",
    "ending_type": "结尾类型",
    "length_category": "短/中/长"
  },
  
  "emotion": {
    "primary": "主情感",
    "intensity": 3,
    "tone": "语气类型"
  },
  
  "topics": ["主题1", "主题2"],
  
  "serendipity_seeds": ["种子词1", "种子词2", "种子词3"]
}
```

## 最终索引文件格式

合并后保存到 `.mirror-writing/_index.json`：

```json
{
  "version": "1.0",
  "created_at": "日期",
  "total_files": 数量,
  "files": [
    // 所有单文件分析结果
  ],
  "global_index": {
    "keyword_to_files": {"关键词A": ["001", "003"], ...},
    "topic_to_files": {"职场": ["001", "002"], ...},
    "emotion_to_files": {"励志": ["001", "005"], ...},
    "seed_to_files": {"种子词A": ["002", "007"], ...}
  }
}
```

## 增量更新

当有新文件加入时：
1. 读取现有 _index.json
2. 识别新增文件（对比文件列表）
3. 仅分析新文件（可能需要分批）
4. 追加到 files 数组
5. 更新 global_index 反向索引
6. 更新 total_files 计数

## 清理临时文件

合并生成 `_index.json` 后，自动删除临时的批次文件：

保留最终的 `_index.json` 即可。